// This file was generated by lezer-generator. You probably shouldn't edit it.
import {Parser} from "lezer"
import {scanLineStart, scanThematicBreak, eatAnything} from "./tokens"
import {NodeProp} from "lezer"
export const parser = Parser.deserialize({
  states: "&rOPOgOOOvOgO'#CnOOOS'#C_'#C_OOOS'#C`'#C`OOOS'#Ca'#CaOOOS'#Cb'#CbOOOS'#Cc'#CcOOOS'#Cd'#CdO!mOWO'#C^O!qOfO'#CfO#[OTO'#CfO#cOgO'#CoO#[OTO'#ChO$YOaO'#CkOOOS'#Cg'#CgO$aOgO'#CpOOOS(3AO(3AOOOOS'#Ci'#CiO%WOgO'#CqOOOS(3AP(3APOOOS'#Cy'#CyOOOS(3@|(3@|QOOOOOO%}OSO'#CzOOOS,59U,59UOOOS'#Ce'#CeO#[OTO,58xO!mOWO,58xO#[OTO,59QO#[OTO,59SOOOS'#DT'#DTOOOS,59Q,59QOOOS,59V,59VOOOS,59S,59SO&RO`O'#ChOOOS-E6i-E6iOOOS,59W,59WOOOS,59X,59XOOOW,59f,59fOOOS1G.d1G.dO#[OTO1G.dOOOS1G.l1G.lOOOS1G.n1G.nOOOS7+$O7+$O",
  stateData: "&V~gXOh]OicOkYOl[OoQOqROrSOsTOtUOuVOfbP~gXOh]OicOkYOl[OoQOqROrSOsTOtUOuVOfbX~viO~klOlmOoQOqROrSOsTOtUOuVO~jnOxnO~h]OfcXgcXicXkcXlcXocXqcXrcXscXtcXucX~grOl[O~grOl[OfdXhdXidXkdXodXqdXrdXsdXtdXudX~icOfeXgeXheXkeXleXoeXqeXreXseXteXueX~pvO~lmO~",
  goto: "#wxPPy}}}}}}!Syy!Yy!b!h!o!v!}#Q#UyPPPPPPP#Y#^PPPPPPPP#eTdOPVgOPXQjWRxkU`OP_Rs]QPORhPSZOPRpZS_OPRt_SbOPRubRfOT^OPTaOPTeOPSWOPRkXQoYQq[QwjQylQzmR{x",
  nodeNames: "âš  Document AtxHeading AtxHeadingMarker1 AtxHeadingMarker2 AtxHeadingMarker3 AtxHeadingMarker4 AtxHeadingMarker5 AtxHeadingMarker6 AtxHeadingText ThematicBreak IndentedCodeBlock Line GenericBlock",
  nodeProps: [
    [NodeProp.top, 1,true]
  ],
  repeatNodeCount: 4,
  tokenData: "$h~RVXYhYZs]^xpq!Qqs!bst!jt~!bPmQpPXYhpqh~xOx~~}Px~YZsR!XRpPvQXYhpq!Qq~!bQ!gPvQp~!bR!qRoPvQps!bst!zt~!bR#RRqPvQps!bst#[t~!bR#cRrPvQps!bst#lt~!bR#sRsPvQps!bst#|t~!bR$TRtPvQps!bst$^t~!bR$ePuPvQp~!b",
  tokenizers: [scanLineStart, scanThematicBreak, 0, 1, eatAnything],
  topRules: {"Document":[0,1]},
  specializeTable: 0,
  tokenPrec: 0
})
